{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Experiment Tracking with CNNs on MNIST\n",
    "\n",
    "This notebook demonstrates how to train a Convolutional Neural Network (CNN) on the MNIST dataset and track the training process using **Comet.ml**. The goal is to experiment with different architectures and hyperparameters, log the results, and analyze the performance of the models.\n",
    "\n",
    "## Sections:\n",
    "1. **Imports**: Load the necessary libraries and set up the environment.\n",
    "2. **Dataset Preparation**: Load and preprocess the MNIST dataset.\n",
    "3. **Plot Random Images**: Plot 36 random images from MNIST dataset.\n",
    "4. **Model Building**: Define the Fully Connected Neural Network (FCNN) and CNN architectures.\n",
    "5. **Experiment Tracking**: Use Comet.ml to log hyperparameters, metrics, and results.\n",
    "6. **Training and Evaluation**: Train the models and evaluate their performance on the test dataset.\n",
    "7. **Results**: Visualize the results and predictions.\n",
    "8. **Trying Best model**: We use the best-performing model to make predictions on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this section, we import the required libraries for the notebook. Here's a breakdown of the key imports:\n",
    "\n",
    "- **`dotenv`**: Used to load environment variables from a `.env` file. This is particularly useful for securely storing sensitive information like the Comet API key.\n",
    "- **`comet_ml`**: Enables experiment tracking and logging to the Comet.ml platform.\n",
    "- **`tensorflow`**: Provides tools for building and training neural networks.\n",
    "- **`matplotlib` and `numpy`**: Used for data visualization and numerical operations.\n",
    "- **`cv2`**: Used for image preprocessing when working with custom input images.\n",
    "\n",
    "### Why Use `dotenv`?\n",
    "The `dotenv` library allows us to securely load the Comet API key from a `.env` file instead of hardcoding it into the notebook. This ensures that sensitive information is not exposed in the code and can be easily managed across different environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import comet_ml\n",
    "\n",
    "# Import Tensorflow 2.0 - \n",
    "# pip install tensorflow-macos (optimized version)\n",
    "# pip install tensorflow-metal (GPU acceleration)\n",
    "import tensorflow as tf\n",
    "\n",
    "# other packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ipywidgets as widgets\n",
    "from ipycanvas import Canvas\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we are using a GPU, if not switch runtimes\n",
    "# using Runtime > Change Runtime Type > GPU\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "# load and set up comet_ml api key\n",
    "load_dotenv()\n",
    "COMET_API_KEY = os.getenv(\"COMET_API_KEY\")\n",
    "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "The MNIST dataset is a collection of 70,000 grayscale images of handwritten digits (0-9), commonly used for training and testing machine learning models. Each image is 28x28 pixels in size.\n",
    "\n",
    "### Dataset Structure:\n",
    "- **Training Set**: 60,000 images and their corresponding labels.\n",
    "- **Test Set**: 10,000 images and their corresponding labels.\n",
    "\n",
    "When we load the dataset using TensorFlow's `tf.keras.datasets.mnist.load_data()` function, it returns two tuples:\n",
    "1. `(train_images, train_labels)`: The training images and their labels.\n",
    "2. `(test_images, test_labels)`: The test images and their labels.\n",
    "\n",
    "### Shape of the Data:\n",
    "- `train_images.shape`: `(60000, 28, 28)`  \n",
    "  This means there are 60,000 training images, each of size 28x28 pixels.\n",
    "- `test_images.shape`: `(10000, 28, 28)`  \n",
    "  This means there are 10,000 test images, each of size 28x28 pixels.\n",
    "\n",
    "### Preparing the Dataset:\n",
    "Before feeding the data into a Convolutional Neural Network (CNN), we need to preprocess it. This involves two key steps:\n",
    "\n",
    "1. **Normalization**:\n",
    "   - The pixel values in the images range from 0 to 255. We normalize these values to the range [0, 1] by dividing each pixel value by 255.0.\n",
    "   - Normalization helps the model train faster and improves numerical stability.\n",
    "\n",
    "2. **Adding an Extra Channel Dimension**:\n",
    "   - The original images are 2D arrays of shape `(28, 28)`. CNNs, however, expect input images to have an additional channel dimension, typically used for color channels (e.g., RGB).\n",
    "   - Since MNIST images are grayscale, we add a single channel dimension to make the shape `(28, 28, 1)`. This is done using `np.expand_dims()`.\n",
    "\n",
    "### Why Add the Extra Channel Dimension?\n",
    "- The extra channel dimension is required because CNNs are designed to process multi-channel images (e.g., RGB images with 3 channels).\n",
    "- For grayscale images like MNIST, the single channel dimension indicates that there is only one intensity value per pixel.\n",
    "\n",
    "### Final Dataset Shapes:\n",
    "- `train_images.shape`: `(60000, 28, 28, 1)`  \n",
    "  60,000 training images, each of size 28x28 with 1 channel.\n",
    "- `test_images.shape`: `(10000, 28, 28, 1)`  \n",
    "  10,000 test images, each of size 28x28 with 1 channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist\n",
    "\n",
    "print(\"Training images shape: \", train_images.shape)\n",
    "print(\"Training labels shape: {}\".format(train_labels.shape))\n",
    "print(\"Testing images shape: {}\".format(tf.shape(test_images)))\n",
    "print(f\"Test labels shape: {test_labels.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATASET FOR CNN\n",
    "# Adjust the shape of the images and normalize pixel values in the range [0,1]\n",
    "print(f\"{' Not normalized image values ':-^100}\")\n",
    "print(train_images[0])\n",
    "# Mnist dataset contains images of size 28x28 and pixel values in the range [0,255]\n",
    "\n",
    "train_images = (np.expand_dims(train_images, axis = -1)/255.0).astype(np.float32)\n",
    "test_images = (np.expand_dims(test_images, axis = -1)/255.0).astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int64)\n",
    "test_labels = test_labels.astype(np.int64)\n",
    "\n",
    "print(f\"{' Printing train_images values ':-^100}\")\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Random Images\n",
    "\n",
    "In this section, we visualize 36 randomly selected images from the MNIST training dataset. This helps us understand the structure of the data and verify that the images are loaded correctly.\n",
    "\n",
    "### How It Works:\n",
    "1. **Random Selection of Images**:\n",
    "   - We use the `np.random.choice()` function to randomly select 36 indices from the training dataset without replacement. This ensures that the same image is not selected more than once.\n",
    "   - Example:\n",
    "     ```python\n",
    "     random_indices = np.random.choice(train_images.shape[0], 36, replace=False)\n",
    "     ```\n",
    "     - `train_images.shape[0]`: The total number of training images (60,000).\n",
    "     - `36`: The number of random indices to select.\n",
    "     - `replace=False`: Ensures that the same index is not selected multiple times.\n",
    "\n",
    "2. **Plotting the Images**:\n",
    "   - We use `matplotlib` to create a 6x6 grid of subplots, where each subplot displays one of the randomly selected images.\n",
    "   - The `cmap=plt.cm.binary` argument is used to display the images in grayscale. This ensures that the pixel intensity values (ranging from 0 to 1 after normalization) are represented as shades of black and white.\n",
    "\n",
    "### Why Use `cmap=plt.cm.binary`?\n",
    "- The MNIST dataset contains grayscale images, so using the `binary` colormap ensures that the images are displayed in their original format (black and white).\n",
    "- Without specifying `cmap`, the images might be displayed using a default colormap, which could add unnecessary colors.\n",
    "\n",
    "### Visualization:\n",
    "Each subplot shows:\n",
    "- The image of the handwritten digit.\n",
    "- The corresponding label (actual digit) and the index of the image in the dataset.\n",
    "\n",
    "This visualization provides a quick overview of the dataset and helps verify that the data preprocessing steps (e.g., normalization and adding the channel dimension) were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 36 random images from the dataset\n",
    "print(f\"{ ' Plotting 36 random images ':-^100}\")\n",
    "\n",
    "print(f\"{' Randomly selected images ':-^100}\")\n",
    "random_indices = np.random.choice(train_images.shape[0],36,replace = False)\n",
    "print(f\"Random indices: {random_indices}\")\n",
    "\n",
    "print(f\"{' Plotting the images ':-^100}\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[random_indices[i]], cmap = plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "    title = f\"Label: {train_labels[random_indices[i]]} \\n Index: {random_indices[i]}\"\n",
    "    plt.title(title)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.suptitle(\"MNIST Dataset - Randomly Selected Images\", fontsize=16)\n",
    "\n",
    "print(f\"{' Logging the images to Comet ':-^100}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "In this section, we define the hyperparameters and experiments for training our models. Each experiment is represented as a dictionary containing the following details:\n",
    "- **Architecture**: The type of neural network to use (FCNN or CNN).\n",
    "- **Optimizer**: The optimization algorithm for training (e.g., SGD or Adam).\n",
    "- **Learning Rate**: The step size for updating weights during training.\n",
    "- **Batch Size**: The number of samples processed before updating the model.\n",
    "- **Number of Epochs**: The number of times the model will see the entire dataset during training.\n",
    "- **Loss Function**: The function used to measure the error between predictions and actual labels.\n",
    "\n",
    "### Two Main Architectures:\n",
    "1. **FCNN (Fully Connected Neural Network)**:\n",
    "   - This is a simple type of neural network where every neuron in one layer is connected to every neuron in the next layer.\n",
    "   - It works well for structured data but may struggle with image data because it doesn't take spatial relationships (like pixel positions) into account.\n",
    "\n",
    "2. **CNN (Convolutional Neural Network)**:\n",
    "   - This is a more advanced type of neural network designed specifically for image data.\n",
    "   - It uses convolutional layers to detect patterns like edges, shapes, and textures in images, making it highly effective for tasks like image classification.\n",
    "\n",
    "### Experiment Setup:\n",
    "We define multiple experiments to test different combinations of architectures, optimizers, and learning rates. This allows us to compare their performance and find the best configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE HYPERPARAMETERES + Experiment creation\n",
    "\n",
    "# Define all the experiments and their hyperparameters\n",
    "experiments = [\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.05, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.01, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    {\"arquitecture\": \"CNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.0005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.0001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.0005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.0001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"FCNN\", \"optimizer\": \"Adam\", \"learning_rate\": 0.001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.05, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.01, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.005, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "    # {\"arquitecture\": \"CNN\", \"optimizer\": \"SGD\", \"learning_rate\": 0.001, \"batch_size\": 64, \"num_epochs\": 5, \"loss_function\": \"sparse_categorical_crossentropy\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding FCNN and CNN Networks\n",
    "\n",
    "In this notebook, we use two types of neural networks: **Fully Connected Neural Networks (FCNN)** and **Convolutional Neural Networks (CNN)**. Here's a simple explanation of how they work and why certain design choices are made.\n",
    "\n",
    "### Fully Connected Neural Network (FCNN)\n",
    "- An FCNN is a basic type of neural network where every neuron in one layer is connected to every neuron in the next layer.\n",
    "- It processes data in a flat, one-dimensional format, which means it doesn't take into account the spatial structure of images (e.g., the arrangement of pixels).\n",
    "- FCNNs are simple and work well for structured data but are less effective for image data because they ignore spatial relationships.\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "- A CNN is specifically designed for image data. It uses **convolutional layers** to detect patterns like edges, shapes, and textures in images.\n",
    "- CNNs preserve the spatial structure of images, making them much better suited for tasks like image classification.\n",
    "\n",
    "### Key Components of CNN:\n",
    "1. **Convolutional Layers**:\n",
    "   - These layers apply filters (small matrices) to the input image to detect patterns.\n",
    "   - The number of filters increases from the first to the second convolutional layer because:\n",
    "     - The first layer detects simple patterns like edges.\n",
    "     - The second layer combines these simple patterns to detect more complex features like shapes or textures.\n",
    "\n",
    "2. **Max Pooling Layers**:\n",
    "   - These layers reduce the size of the feature maps by taking the maximum value in small regions.\n",
    "   - This helps reduce the computational cost and makes the model more robust to small changes in the input.\n",
    "\n",
    "3. **Flatten Layer**:\n",
    "   - After the convolutional and pooling layers, the feature maps are still in a 2D format.\n",
    "   - The **Flatten layer** converts these 2D feature maps into a 1D vector so they can be passed to the fully connected layers for classification.\n",
    "\n",
    "4. **Fully Connected Layers**:\n",
    "   - These layers take the flattened vector and learn to classify the image based on the features extracted by the convolutional layers.\n",
    "\n",
    "5. **Softmax Activation Function**:\n",
    "   - The output layer uses the **softmax activation function** because:\n",
    "     - It converts the raw output values (logits) into probabilities.\n",
    "     - Each probability represents the likelihood of the input belonging to a specific class (e.g., digits 0-9 in the MNIST dataset).\n",
    "     - The sum of all probabilities is 1, making it easy to interpret the model's predictions.\n",
    "\n",
    "### Why These Design Choices?\n",
    "- **Increasing Filters**: By increasing the number of filters in deeper layers, the model can learn more complex and abstract features, which improves its ability to classify images accurately.\n",
    "- **Flatten Layer**: This is necessary to bridge the gap between the convolutional layers (which work with 2D data) and the fully connected layers (which require 1D data).\n",
    "- **Softmax Activation**: It ensures the model outputs probabilities, making it easier to interpret and evaluate the predictions.\n",
    "\n",
    "By combining these components, CNNs are able to effectively process and classify image data, making them a powerful tool for computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MODELS - FCNN OR CNN\n",
    "\n",
    "def build_model(arquitecture):\n",
    "    if arquitecture == \"FCNN\":\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    elif arquitecture == \"CNN\":\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape = (28,28,1)),\n",
    "            tf.keras.layers.Conv2D(24, kernel_size=(3,3),activation = 'relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Conv2D(36,kernel_size=(3,3),activation = 'relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(10, activation = 'softmax'),\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Tracking\n",
    "\n",
    "The `create_experiment()` function is used to set up and manage experiments with **Comet.ml**. This function helps us log important details about each training run, such as the architecture, optimizer, and learning rate, so we can track and compare the performance of different configurations.\n",
    "\n",
    "### How It Works:\n",
    "1. **End Previous Experiments**:\n",
    "   - If there is an existing experiment running, it is ended to ensure a clean start for the new experiment.\n",
    "\n",
    "2. **Initialize a New Experiment**:\n",
    "   - A new Comet experiment is created using the API key and project name. This allows us to log data to the Comet.ml platform.\n",
    "\n",
    "3. **Log Parameters**:\n",
    "   - The function logs key parameters like the architecture (e.g., FCNN or CNN)\n",
    "\n",
    "4. **Set Experiment Name**:\n",
    "   - The experiment is given a unique name based on the optimizer and learning rate, making it easier to identify in the Comet.ml dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Comet experiment function to track each of the training runs\n",
    "\n",
    "comet_ml.login(\n",
    "    api_key=COMET_API_KEY,\n",
    ")\n",
    "\n",
    "def create_experiment(arquitecture,optimizer, learning_rate):\n",
    "  # initiate the comet experiment for tracking\n",
    "  experiment_config = comet_ml.ExperimentConfig(\n",
    "      name = optimizer + \"_\" + learning_rate,\n",
    "      tags = [arquitecture],\n",
    "  )\n",
    "\n",
    "  experiment = comet_ml.start(\n",
    "                  api_key=COMET_API_KEY,\n",
    "                  project_name=\"mnist-digits\",\n",
    "                  mode=\"create\",\n",
    "                  experiment_config=experiment_config,\n",
    "  )\n",
    "  \n",
    "  return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "The `run_experiment()` function is responsible for running each experiment defined in the previous steps. It loops over all the experiments, builds the model, trains it, evaluates it, and logs the results using **Comet.ml**.\n",
    "\n",
    "### How It Works:\n",
    "1. **Create a Comet Experiment**:\n",
    "   - For each experiment, we call the `create_experiment()` function to set up a new experiment in Comet.ml for tracking.\n",
    "\n",
    "2. **Build the Model**:\n",
    "   - The model is built based on the architecture specified in the experiment (either FCNN or CNN).\n",
    "\n",
    "3. **Compile the Model**:\n",
    "   - The `model.compile()` method is used to configure the model for training. It specifies:\n",
    "     - **Loss Function**: Measures the error between the predicted and actual labels.\n",
    "     - **Optimizer**: Determines how the model updates its weights during training.\n",
    "     - **Metrics**: Specifies what metrics (e.g., accuracy) to track during training.\n",
    "\n",
    "4. **Train the Model**:\n",
    "   - The `model.fit()` method is used to train the model. It takes the training data and trains the model for a specified number of epochs. Key parameters include:\n",
    "     - **Batch Size**: The number of samples processed before updating the model's weights.\n",
    "     - **Epochs**: The number of times the model sees the entire dataset during training.\n",
    "\n",
    "5. **Evaluate the Model**:\n",
    "   - The `model.evaluate()` method is used to test the model on the test dataset. It calculates the loss and accuracy on unseen data.\n",
    "\n",
    "6. **Log Metrics**:\n",
    "   - The test loss and accuracy are logged to Comet.ml for tracking and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 RUN EXPERIMENT\n",
    "\n",
    "\n",
    "def run_experiment(exp, train_images, train_labels, test_images, test_labels):\n",
    "    # CREATE COMET EXPERIMENT\n",
    "    # experiment = create_experiment(exp[\"arquitecture\"], exp[\"optimizer\"], str(exp[\"learning_rate\"]))\n",
    "\n",
    "    # BUILD MODEL\n",
    "    model = build_model(exp[\"arquitecture\"])\n",
    "\n",
    "    # COMPILE\n",
    "    model.compile(\n",
    "        loss = exp[\"loss_function\"],\n",
    "        optimizer = tf.keras.optimizers.get(\n",
    "            {\n",
    "                \"class_name\": exp[\"optimizer\"],\n",
    "                \"config\": {\n",
    "                    \"learning_rate\": exp[\"learning_rate\"],\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    # TRAIN\n",
    "    model.fit(train_images, train_labels,\n",
    "            batch_size = exp[\"batch_size\"],\n",
    "            epochs = exp[\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "    # EVALUATE\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels, batch_size = exp[\"batch_size\"])\n",
    "\n",
    "    # SAVE MODEL AND LOG METRICS\n",
    "    model_path = \"model.keras\"\n",
    "    model.save(model_path)\n",
    "\n",
    "    # LOG MODEL TO COMET\n",
    "    # experiment.log_model(experiment.name, model_path)\n",
    "\n",
    "    # experiment.log_metric(\"test_loss\", test_loss)\n",
    "    # experiment.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    # experiment.end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for exp in experiments:\n",
    "    run_experiment(exp, train_images, train_labels, test_images, test_labels)\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     for exp in experiments:\n",
    "#         executor.submit(run_experiment, exp, train_images, train_labels, test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### 1. Overfitting\n",
    "- When a machine learning model performs worse on new data (test dataset) than on the training data, it is an example of **overfitting**.\n",
    "- Overfitting occurs when the model learns patterns specific to the training data but fails to generalize to unseen data.\n",
    "- This is observed when the accuracy on the test dataset is lower than the accuracy on the training dataset.\n",
    "\n",
    "### 2. Maximum Accuracy with FCNN\n",
    "- The maximum accuracy achieved on the training dataset using a **Fully Connected Neural Network (FCNN)** is **0.92**.\n",
    "- This result is obtained using the **Adam optimizer** with a learning rate of **0.005**.\n",
    "\n",
    "### 3. Maximum Accuracy with CNN\n",
    "- The maximum accuracy achieved on the training dataset using a **Convolutional Neural Network (CNN)** is **0.9846**.\n",
    "- This result is accomplished using the **Adam optimizer** with a learning rate of **0.0005**.\n",
    "- CNNs outperform FCNNs because they are better suited for image data, as they preserve spatial relationships and extract hierarchical features.\n",
    "\n",
    "### 4. Impact of Large Learning Rates\n",
    "- It is important to note that setting excessively large learning rates can cause the model to diverge and increase the loss substantially.\n",
    "- For example:\n",
    "  - Using **Adam** with a learning rate of **0.005** or **SGD** with a learning rate of **0.1** can lead to unstable training and poor performance.\n",
    "- Proper tuning of the learning rate is crucial to ensure stable and effective training.\n",
    "\n",
    "### Key Takeaways\n",
    "- CNNs are more effective than FCNNs for image classification tasks, achieving higher accuracy.\n",
    "- Careful selection of hyperparameters, such as the optimizer and learning rate, is critical for achieving optimal performance.\n",
    "- Monitoring the difference between training and test accuracy helps identify overfitting and guides improvements in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Best Model\n",
    "\n",
    "After training, we use the best-performing model (CNN with **Adam optimizer** and a learning rate of **0.0005**) to make predictions on new data. This model achieved the highest accuracy during training and is well-suited for classifying handwritten digits from the MNIST dataset.\n",
    "\n",
    "### Steps:\n",
    "1. **Load the Test Image**:\n",
    "   - The test image is preprocessed to match the input format expected by the model (28x28 pixels, normalized, and with a single channel).\n",
    "\n",
    "2. **Make the Prediction**:\n",
    "   - The model's `predict()` method is used to generate probabilities for each class (digits 0-9).\n",
    "   - The class with the highest probability is selected as the predicted label.\n",
    "\n",
    "3. **Visualize the Result**:\n",
    "   - The test image is displayed alongside the predicted label for easy interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 MAKE PREDICTION WITH CNN MODEL\n",
    "\n",
    "# Download a specific model from Comet\n",
    "comet_ml.login(api_key=COMET_API_KEY,)\n",
    "api = comet_ml.API()\n",
    "model = api.get_model(\n",
    "    workspace=api.get_default_workspace(), model_name=\"adam_0.0005\",\n",
    ")\n",
    "model.download(\"1.0.0\", output_folder=\"./\", expand=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the downloaded model to make predictions\n",
    "model = tf.keras.models.load_model(\"model.keras\")\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the classification results\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "random_indices = np.random.choice(test_images.shape[0],36,replace = False)\n",
    "\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(test_images[random_indices[i]], cmap = plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted label: {np.argmax(predictions[random_indices[i]])} \\n Actual label: {test_labels[random_indices[i]]}\", fontsize = 8)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.suptitle(\"MNIST Dataset - Randomly Selected Images\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas\n",
    "from ipywidgets import Button, VBox, Label, Layout\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a smaller canvas (200x200)\n",
    "canvas_size = 200\n",
    "canvas = Canvas(width=canvas_size, height=canvas_size, sync_image_data=True, layout=Layout(border='3px solid black'))\n",
    "canvas.layout.width = '200px'  # Resize display width\n",
    "canvas.line_width = 20\n",
    "canvas.stroke_style = 'black'\n",
    "\n",
    "# Internal state to control drawing only when mouse is pressed\n",
    "is_drawing = {'active': False}\n",
    "\n",
    "# Label for prediction output\n",
    "prediction_label = Label(value=\"Draw a digit and click 'Predict'\")\n",
    "\n",
    "# Buttons\n",
    "predict_button = Button(description=\"Predict\")\n",
    "clear_button = Button(description=\"Clear\")\n",
    "\n",
    "# --- Drawing Control ---\n",
    "def on_mouse_down(x, y):\n",
    "    is_drawing['active'] = True\n",
    "    canvas.begin_path()\n",
    "    canvas.move_to(x, y)\n",
    "\n",
    "def on_mouse_up(x, y):\n",
    "    is_drawing['active'] = False\n",
    "\n",
    "def on_mouse_move(x, y):\n",
    "    if is_drawing['active']:\n",
    "        canvas.line_to(x, y)\n",
    "        canvas.stroke()\n",
    "        canvas.begin_path()\n",
    "        canvas.move_to(x, y)\n",
    "\n",
    "canvas.on_mouse_down(on_mouse_down)\n",
    "canvas.on_mouse_up(on_mouse_up)\n",
    "canvas.on_mouse_move(on_mouse_move)\n",
    "\n",
    "# --- Clear Function ---\n",
    "def clear_canvas(b):\n",
    "    canvas.clear()\n",
    "    # Force redraw to prevent ghost image\n",
    "    canvas.put_image_data(np.ones((canvas_size, canvas_size, 4), dtype=np.uint8) * 255, 0, 0)\n",
    "    canvas.clear()\n",
    "    prediction_label.value = \"Canvas cleared. Draw again.\"\n",
    "\n",
    "clear_button.on_click(clear_canvas)\n",
    "\n",
    "# --- Prediction Function ---\n",
    "def predict_digit(b):\n",
    "    # Ensure sync_image_data=True when creating the canvas\n",
    "    data = np.array(canvas.get_image_data())  # shape: (height, width, 4)\n",
    "\n",
    "    # Convert RGBA to grayscale using the alpha channel\n",
    "    grayscale = data[:, :, 3]  # Invert alpha channel to get black-on-white\n",
    "\n",
    "    # Convert to PIL image\n",
    "    img = Image.fromarray(grayscale.astype('uint8'), mode='L')\n",
    "    img = img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Normalize and reshape for model input\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)\n",
    "\n",
    "    plt.imshow(img_array[0, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_digit = np.argmax(prediction)\n",
    "\n",
    "    # Show prediction and the resized image\n",
    "    prediction_label.value = f\"Predicted digit: {predicted_digit}\"\n",
    "\n",
    "predict_button.on_click(predict_digit)\n",
    "\n",
    "# --- Display Everything ---\n",
    "display(VBox([canvas, predict_button, clear_button, prediction_label]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Now build the activation model\n",
    "conv_layers_outputs = [layer.output for layer in model.layers if isinstance(layer, Conv2D)]\n",
    "activation_model = Model(inputs=model.layers[0].input, outputs=conv_layers_outputs)\n",
    "\n",
    "# Use a sample test image\n",
    "img = test_images[np.random.choice(test_images.shape[0], 1, replace=False)]\n",
    "\n",
    "# Ensure image has the right shape\n",
    "if img.ndim == 3:\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Run prediction and get activations\n",
    "prediction = model.predict(img)\n",
    "activations = activation_model.predict(img)\n",
    "\n",
    "for idx, layer_activation in enumerate(activations):\n",
    "    print(f\"Layer {idx} output shape: {layer_activation.shape}\")\n",
    "\n",
    "# Plot feature maps\n",
    "for layer_activation in activations:\n",
    "    num_filters = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    display_grid = np.zeros((size, size * num_filters))\n",
    "    for i in range(num_filters):\n",
    "        x = layer_activation[0, :, :, i]\n",
    "        x -= x.mean()\n",
    "        x /= (x.std() + 1e-5)\n",
    "        x = np.clip(x*100+128, 0, 255).astype('uint8')\n",
    "        display_grid[:, i * size:(i + 1) * size] = x\n",
    "\n",
    "    scale = 1.0 / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(\"Feature Maps\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='gray')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
